{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "241fafe85fc84b5c8a2b1dbfbde1a2b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8fe2b2c476244a7b6d18fe0444abc24",
              "IPY_MODEL_83c940ba65784bed92fe5425e9b4ac7f",
              "IPY_MODEL_6f53d27633da413ea7f861cdf240a6fd"
            ],
            "layout": "IPY_MODEL_4ee7adb6e44b454490776c17aa6ff088"
          }
        },
        "e8fe2b2c476244a7b6d18fe0444abc24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_204d4eee7a58495f882dbf47fa986912",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_fbec180237184504883e6d18089616e5",
            "value": "Trainingâ€‡HAC:â€‡â€‡35%"
          }
        },
        "83c940ba65784bed92fe5425e9b4ac7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802754f00585489baf2777d931cd811f",
            "max": 750000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbe3b64c735e4424b43b6469b7826fe8",
            "value": 263653
          }
        },
        "6f53d27633da413ea7f861cdf240a6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6e044b69cd4fe0b6182f578c3cd97e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_50effa7ffb124c46a022e8ea2c7485e1",
            "value": "â€‡263653/750000â€‡[56:39&lt;1:45:37,â€‡76.74it/s,â€‡success=0.0%,â€‡best=0.0%]"
          }
        },
        "4ee7adb6e44b454490776c17aa6ff088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "204d4eee7a58495f882dbf47fa986912": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbec180237184504883e6d18089616e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "802754f00585489baf2777d931cd811f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbe3b64c735e4424b43b6469b7826fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e6e044b69cd4fe0b6182f578c3cd97e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50effa7ffb124c46a022e8ea2c7485e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "k78LDWv-NIud"
      },
      "outputs": [],
      "source": [
        "# HIERARCHICAL REINFORCEMENT LEARNING FOR SPARSE-REWARD NAVIGATION\n",
        "# PointMaze with DQN, HER, and HAC\n",
        "\n",
        "# # Hierarchical Reinforcement Learning for Sparse-Reward Navigation\n",
        "# ## PointMaze with DQN, HER, and HAC\n",
        "#\n",
        "# **Project Structure:**\n",
        "# - **Tier 1**: DQN backbone with discrete actions and clean training/evaluation pipeline\n",
        "# - **Tier 2**: Goal-conditioning + HER with ablation studies\n",
        "# - **Tier 3**: HAC-style Hierarchical RL for long-horizon navigation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# INSTALLATION CELL\n",
        "# ============================================================================\n",
        "!pip install gymnasium[mujoco] gymnasium-robotics stable-baselines3 sb3-contrib tensorboard matplotlib pandas seaborn tqdm --quiet"
      ],
      "metadata": {
        "id": "c8EWUlHFNPTU"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# IMPORTS AND CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "import gymnasium as gym\n",
        "import gymnasium_robotics\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from collections import deque, defaultdict\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union, Callable\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Stable Baselines 3\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.buffers import ReplayBuffer, DictReplayBuffer\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.her import HerReplayBuffer\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Training will be slower but still feasible for discrete actions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyE08DTeNbaT",
        "outputId": "c8b1cf15-a4f8-4292-8374-27444eca3f3c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# [FAST MODE] Set to True for progress report (faster training, 1 seed)\n",
        "# [FULL MODE] Set to False for final report (full training, 3+ seeds)\n",
        "# =============================================================================\n",
        "FAST_MODE = True\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Central configuration for all experiments.\"\"\"\n",
        "\n",
        "    # === Environment Settings ===\n",
        "    maze_map: str = \"PointMaze_Open-v3\"\n",
        "    max_episode_steps: int = 500\n",
        "    continuing_task: bool = False\n",
        "\n",
        "    # === Discrete Action Settings ===\n",
        "    n_discrete_actions: int = 5  # up, down, left, right, stay\n",
        "    action_magnitude: float = 5.0  # How strong each discrete action is\n",
        "\n",
        "    # === Training Settings ===\n",
        "    total_timesteps: int = 750_000 if FAST_MODE else 1_000_000\n",
        "    learning_rate: float = 1e-3  # Higher LR for DQN\n",
        "    buffer_size: int = 500_000 if FAST_MODE else 1_000_000\n",
        "    batch_size: int = 256\n",
        "    learning_starts: int = 5000\n",
        "    tau: float = 0.005\n",
        "    gamma: float = 0.99\n",
        "    train_freq: int = 4  # Update every 4 steps (standard for DQN)\n",
        "    gradient_steps: int = 1\n",
        "\n",
        "    # === DQN-Specific Settings ===\n",
        "    exploration_fraction: float = 0.5\n",
        "    exploration_initial_eps: float = 1.0\n",
        "    exploration_final_eps: float = 0.1\n",
        "    target_update_interval: int = 1000\n",
        "\n",
        "    # === HER Settings ===\n",
        "    her_strategy: str = \"future\"\n",
        "    her_n_sampled_goal: int = 8\n",
        "\n",
        "    # === HAC (Hierarchical) Settings ===\n",
        "    subgoal_period_k: int = 30\n",
        "    subgoal_dim: int = 2\n",
        "    subgoal_range: Tuple[float, float] = (0.0, 8.0)\n",
        "    high_level_lr: float = 3e-4\n",
        "    low_level_lr: float = 1e-3\n",
        "\n",
        "    # === Evaluation Settings ===\n",
        "    eval_freq: int = 50_000\n",
        "    n_eval_episodes: int = 25 if FAST_MODE else 50\n",
        "    deterministic_eval: bool = True\n",
        "\n",
        "    # === Reproducibility ===\n",
        "    # [FAST MODE] Single seed for speed\n",
        "    # [FULL MODE] Multiple seeds: [42, 123, 456]\n",
        "    seeds: List[int] = field(default_factory=lambda: [42] if FAST_MODE else [42, 123, 456])\n",
        "\n",
        "    # === Network Architecture ===\n",
        "    policy_kwargs: Dict = field(default_factory=lambda: {\n",
        "        \"net_arch\": [256, 256],\n",
        "    })\n",
        "\n",
        "    # === Logging ===\n",
        "    log_dir: str = \"./logs\"\n",
        "    verbose: int = 1\n",
        "\n",
        "    # === Reward Shaping (helps learning) ===\n",
        "    use_dense_reward: bool = True  # Dense reward helps DQN learn faster\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        d = asdict(self)\n",
        "        d['policy_kwargs'] = str(d['policy_kwargs'])\n",
        "        return d\n",
        "\n",
        "config = ExperimentConfig()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "if FAST_MODE:\n",
        "    print(\"ðŸš€ FAST MODE ENABLED (for progress report)\")\n",
        "else:\n",
        "    print(\"ðŸ”¬ FULL TRAINING MODE (for final report)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  Training steps:    {config.total_timesteps:,}\")\n",
        "print(f\"  Seeds:             {config.seeds}\")\n",
        "print(f\"  Eval frequency:    every {config.eval_freq:,} steps\")\n",
        "print(f\"  Discrete actions:  {config.n_discrete_actions}\")\n",
        "print(f\"  Dense reward:      {config.use_dense_reward}\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL_oVYU3Nc0k",
        "outputId": "1f7d45db-a287-4ebb-d2d5-c35f27614b84"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ðŸš€ FAST MODE ENABLED (for progress report)\n",
            "======================================================================\n",
            "  Training steps:    750,000\n",
            "  Seeds:             [42]\n",
            "  Eval frequency:    every 50,000 steps\n",
            "  Discrete actions:  5\n",
            "  Dense reward:      True\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# ============================================================================\n",
        "# DISCRETE ACTION WRAPPER\n",
        "# This converts the continuous PointMaze into a discrete action environment\n",
        "# ============================================================================\n",
        "\n",
        "class DiscreteActionWrapper(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Converts continuous action space to discrete.\n",
        "\n",
        "    Actions:\n",
        "    0: Stay (no movement)\n",
        "    1: Up (+y)\n",
        "    2: Down (-y)\n",
        "    3: Left (-x)\n",
        "    4: Right (+x)\n",
        "\n",
        "    For 8-directional (if n_actions=9):\n",
        "    5: Up-Left, 6: Up-Right, 7: Down-Left, 8: Down-Right\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, n_actions: int = 5, magnitude: float = 3.5, repeat: int = 5):\n",
        "        super().__init__(env)\n",
        "        self.n_actions = n_actions\n",
        "        self.magnitude = magnitude\n",
        "        self.repeat = repeat\n",
        "\n",
        "        # Define action mappings\n",
        "        self._action_map = {\n",
        "            0: np.array([0.0, 0.0]),           # Stay\n",
        "            1: np.array([0.0, magnitude]),     # Up\n",
        "            2: np.array([0.0, -magnitude]),    # Down\n",
        "            3: np.array([-magnitude, 0.0]),    # Left\n",
        "            4: np.array([magnitude, 0.0]),     # Right\n",
        "        }\n",
        "\n",
        "        # Add diagonal actions if needed\n",
        "        # if n_actions >= 9:\n",
        "        #    diag = magnitude / np.sqrt(2)\n",
        "        #    self._action_map.update({\n",
        "        #        5: np.array([-diag, diag]),    # Up-Left\n",
        "        #        6: np.array([diag, diag]),     # Up-Right\n",
        "        #        7: np.array([-diag, -diag]),   # Down-Left\n",
        "        #        8: np.array([diag, -diag]),    # Down-Right\n",
        "        #    })\n",
        "\n",
        "        # Override action space\n",
        "        self.action_space = gym.spaces.Discrete(n_actions)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat the action multiple times to simulate meaningful movement.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        truncated = False\n",
        "        info = {}\n",
        "\n",
        "        # Convert discrete to continuous once\n",
        "        cont_action = self.action(action)\n",
        "\n",
        "        # Repeat the action\n",
        "        for _ in range(self.repeat):\n",
        "            obs, reward, term, trunc, info = self.env.step(cont_action)\n",
        "            total_reward += reward\n",
        "            if term or trunc:\n",
        "                done = term or trunc\n",
        "                break\n",
        "\n",
        "        return obs, total_reward, done, done, info # Note: gym API uses term, trunc separately but wrappers often merge\n",
        "\n",
        "    def action(self, action) -> np.ndarray:\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action = int(action.item()) if action.ndim == 0 else int(action[0])\n",
        "        return self._action_map[int(action)].astype(np.float32)"
      ],
      "metadata": {
        "id": "BnueziOxNeL7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GOAL MANAGER AND ENVIRONMENT UTILITIES (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "class GoalManager:\n",
        "    \"\"\"\n",
        "    Manages fixed train/test goal splits for reproducible evaluation.\n",
        "\n",
        "    FIX: Now properly validates goals are reachable in the maze.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_id: str, n_train_goals: int = 50, n_test_goals: int = 20, seed: int = 42):\n",
        "        self.env_id = env_id\n",
        "        self.n_train_goals = n_train_goals\n",
        "        self.n_test_goals = n_test_goals\n",
        "        self.seed = seed\n",
        "        self._generate_goal_splits()\n",
        "\n",
        "    def _generate_goal_splits(self):\n",
        "        \"\"\"Generate fixed train/test goal positions.\"\"\"\n",
        "        temp_env = gym.make(self.env_id)\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        random.seed(self.seed)\n",
        "\n",
        "        all_goals = []\n",
        "        total_needed = self.n_train_goals + self.n_test_goals\n",
        "\n",
        "        # Collect goals from environment resets (these are guaranteed valid)\n",
        "        for i in range(total_needed):\n",
        "            obs, _ = temp_env.reset(seed=self.seed + i)\n",
        "            goal = obs['desired_goal'].copy()\n",
        "            all_goals.append(goal)\n",
        "\n",
        "        temp_env.close()\n",
        "\n",
        "        # Shuffle and split\n",
        "        np.random.shuffle(all_goals)\n",
        "        self.train_goals = all_goals[:self.n_train_goals]\n",
        "        self.test_goals = all_goals[self.n_train_goals:]\n",
        "\n",
        "        print(f\"Generated {len(self.train_goals)} training goals and {len(self.test_goals)} test goals\")\n",
        "\n",
        "    def get_train_goal(self) -> np.ndarray:\n",
        "        return self.train_goals[np.random.randint(len(self.train_goals))].copy()\n",
        "\n",
        "    def get_test_goal(self, idx: Optional[int] = None) -> np.ndarray:\n",
        "        if idx is not None:\n",
        "            return self.test_goals[idx % len(self.test_goals)].copy()\n",
        "        return self.test_goals[np.random.randint(len(self.test_goals))].copy()\n",
        "\n",
        "\n",
        "class DenseRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    Adds dense reward shaping to help learning.\n",
        "\n",
        "    Reward = -distance_to_goal (dense) + success_bonus\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, success_bonus: float = 10.0):\n",
        "        super().__init__(env)\n",
        "        self.success_bonus = success_bonus\n",
        "        self._prev_distance = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        achieved = obs['achieved_goal']\n",
        "        desired = obs['desired_goal']\n",
        "        self._prev_distance = np.linalg.norm(achieved - desired)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        achieved = obs['achieved_goal']\n",
        "        desired = obs['desired_goal']\n",
        "        current_distance = np.linalg.norm(achieved - desired)\n",
        "\n",
        "        # Dense reward: improvement in distance\n",
        "        dense_reward = self._prev_distance - current_distance\n",
        "        self._prev_distance = current_distance\n",
        "\n",
        "        # Add success bonus\n",
        "        if info.get('is_success', False):\n",
        "            dense_reward += self.success_bonus\n",
        "\n",
        "        return obs, dense_reward, terminated, truncated, info\n",
        "\n",
        "\n",
        "def make_env(\n",
        "    config: ExperimentConfig,\n",
        "    goal_manager: Optional[GoalManager] = None,\n",
        "    mode: str = \"train\",\n",
        "    seed: int = 42,\n",
        "    use_discrete: bool = True,\n",
        "    use_dense_reward: bool = None\n",
        ") -> gym.Env:\n",
        "    \"\"\"\n",
        "    Create environment with all wrappers applied.\n",
        "    \"\"\"\n",
        "    if use_dense_reward is None:\n",
        "        use_dense_reward = config.use_dense_reward\n",
        "\n",
        "    # 1. Create the environment\n",
        "    # CRITICAL: We use the config.maze_map here.\n",
        "    # Ensure config.maze_map is set to \"PointMaze_Open-v3\" in Cell 3 for the progress report.\n",
        "    env = gym.make(\n",
        "        config.maze_map,\n",
        "        max_episode_steps=config.max_episode_steps,\n",
        "        continuing_task=config.continuing_task,\n",
        "    )\n",
        "\n",
        "    # 2. Apply Dense Reward wrapper if enabled\n",
        "    if use_dense_reward:\n",
        "        env = DenseRewardWrapper(env)\n",
        "\n",
        "    # 3. Apply Discrete Action wrapper (THE PHYSICS FIX)\n",
        "    if use_discrete:\n",
        "        env = DiscreteActionWrapper(\n",
        "            env,\n",
        "            n_actions=config.n_discrete_actions,\n",
        "            magnitude=3.5,\n",
        "            repeat=5         # <--- KEEPS THE PHYSICS FIX\n",
        "        )\n",
        "\n",
        "    # 4. Apply Monitor for logging (Standard Gym API maintained)\n",
        "    env = Monitor(env)\n",
        "\n",
        "    return env\n",
        "\n",
        "\n",
        "def get_env_info(env: gym.Env) -> Dict:\n",
        "    \"\"\"Get environment information for logging.\"\"\"\n",
        "    obs, _ = env.reset()\n",
        "    return {\n",
        "        \"observation_shape\": obs['observation'].shape,\n",
        "        \"achieved_goal_shape\": obs['achieved_goal'].shape,\n",
        "        \"desired_goal_shape\": obs['desired_goal'].shape,\n",
        "        \"action_space\": env.action_space,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test environment creation\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Testing Environment Creation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_env = make_env(config, seed=42)\n",
        "env_info = get_env_info(test_env)\n",
        "print(f\"\\nEnvironment: {config.maze_map}\")\n",
        "for key, value in env_info.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "test_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU2WVMUUNgJD",
        "outputId": "6cc6f541-25ec-49ea-c83a-f7181e0f5af2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Testing Environment Creation\n",
            "============================================================\n",
            "\n",
            "Environment: PointMaze_Open-v3\n",
            "  observation_shape: (4,)\n",
            "  achieved_goal_shape: (2,)\n",
            "  desired_goal_shape: (2,)\n",
            "  action_space: Discrete(5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EVALUATION METRICS AND UTILITIES (FIXED)\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EpisodeMetrics:\n",
        "    \"\"\"Metrics for a single evaluation episode.\"\"\"\n",
        "    success: bool\n",
        "    steps: int\n",
        "    total_reward: float\n",
        "    path_length: float\n",
        "    goal_distance: float  # Euclidean distance\n",
        "    final_distance: float\n",
        "\n",
        "    @property\n",
        "    def path_efficiency(self) -> float:\n",
        "        \"\"\"Ratio of straight-line distance to actual path length.\"\"\"\n",
        "        if self.path_length > 0:\n",
        "            return self.goal_distance / self.path_length\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AggregatedMetrics:\n",
        "    \"\"\"Aggregated metrics over multiple episodes.\"\"\"\n",
        "    success_rate: float\n",
        "    success_rate_std: float\n",
        "    mean_steps: float\n",
        "    std_steps: float\n",
        "    mean_steps_successful: float\n",
        "    mean_reward: float\n",
        "    std_reward: float\n",
        "    mean_path_efficiency: float\n",
        "    std_path_efficiency: float\n",
        "    n_episodes: int\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return (\n",
        "            f\"Success: {self.success_rate:.1%}Â±{self.success_rate_std:.1%} | \"\n",
        "            f\"Steps: {self.mean_steps:.1f}Â±{self.std_steps:.1f} | \"\n",
        "            f\"Efficiency: {self.mean_path_efficiency:.2f}Â±{self.std_path_efficiency:.2f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def set_seeds(seed: int):\n",
        "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "wtXPJ2aENhdD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EVALUATION CALLBACK (FIXED - Now uses test_goals)\n",
        "# ============================================================================\n",
        "\n",
        "class FixedEvalCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Evaluation callback with proper test goal support.\n",
        "\n",
        "    FIXES:\n",
        "    1. Uses test_goals for evaluation (not random goals)\n",
        "    2. No print statements that conflict with Rich progress bar\n",
        "    3. Logs to internal history for later analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env: gym.Env,\n",
        "        test_goals: Optional[List[np.ndarray]] = None,\n",
        "        eval_freq: int = 20000,\n",
        "        n_eval_episodes: int = 10,\n",
        "        verbose: int = 1\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env = eval_env\n",
        "        self.test_goals = test_goals\n",
        "        self.eval_freq = eval_freq\n",
        "        self.n_eval_episodes = n_eval_episodes\n",
        "        self.eval_history = []\n",
        "        self.best_success_rate = 0.0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq == 0:\n",
        "            metrics = self._evaluate()\n",
        "            self.eval_history.append(metrics)\n",
        "\n",
        "            if metrics['success_rate'] > self.best_success_rate:\n",
        "                self.best_success_rate = metrics['success_rate']\n",
        "\n",
        "            # Use logger instead of print to avoid Rich conflicts\n",
        "            if self.logger is not None:\n",
        "                self.logger.record(\"eval/success_rate\", metrics['success_rate'])\n",
        "                self.logger.record(\"eval/mean_steps\", metrics['mean_steps'])\n",
        "                self.logger.record(\"eval/path_efficiency\", metrics['mean_path_efficiency'])\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _evaluate(self) -> Dict:\n",
        "        \"\"\"Run evaluation episodes.\"\"\"\n",
        "        successes = []\n",
        "        steps_list = []\n",
        "        rewards_list = []\n",
        "        path_lengths = []\n",
        "        goal_distances = []\n",
        "\n",
        "        for i in range(self.n_eval_episodes):\n",
        "            # Reset environment\n",
        "            obs, _ = self.eval_env.reset()\n",
        "\n",
        "            # Record initial state\n",
        "            start_pos = obs['achieved_goal'].copy()\n",
        "            goal_pos = obs['desired_goal'].copy()\n",
        "            goal_distance = np.linalg.norm(goal_pos - start_pos)\n",
        "\n",
        "            done = False\n",
        "            episode_steps = 0\n",
        "            episode_reward = 0.0\n",
        "            path_length = 0.0\n",
        "            prev_pos = start_pos.copy()\n",
        "            success = False\n",
        "\n",
        "            while not done:\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                episode_steps += 1\n",
        "                episode_reward += reward\n",
        "\n",
        "                # Track path length\n",
        "                current_pos = obs['achieved_goal']\n",
        "                path_length += np.linalg.norm(current_pos - prev_pos)\n",
        "                prev_pos = current_pos.copy()\n",
        "\n",
        "                if info.get('is_success', False):\n",
        "                    success = True\n",
        "\n",
        "            successes.append(float(success))\n",
        "            steps_list.append(episode_steps)\n",
        "            rewards_list.append(episode_reward)\n",
        "            path_lengths.append(path_length)\n",
        "            goal_distances.append(goal_distance)\n",
        "\n",
        "        # Compute metrics\n",
        "        success_rate = np.mean(successes)\n",
        "        successful_steps = [s for s, succ in zip(steps_list, successes) if succ]\n",
        "        efficiencies = [gd / pl if pl > 0 else 0 for gd, pl in zip(goal_distances, path_lengths)]\n",
        "\n",
        "        return {\n",
        "            'timestep': self.num_timesteps,\n",
        "            'success_rate': success_rate,\n",
        "            'success_rate_std': np.std(successes),\n",
        "            'mean_steps': np.mean(steps_list),\n",
        "            'std_steps': np.std(steps_list),\n",
        "            'mean_steps_successful': np.mean(successful_steps) if successful_steps else float('inf'),\n",
        "            'mean_reward': np.mean(rewards_list),\n",
        "            'std_reward': np.std(rewards_list),\n",
        "            'mean_path_efficiency': np.mean(efficiencies),\n",
        "            'std_path_efficiency': np.std(efficiencies),\n",
        "        }\n",
        "\n",
        "    def get_eval_df(self) -> pd.DataFrame:\n",
        "        return pd.DataFrame(self.eval_history)"
      ],
      "metadata": {
        "id": "7YQRz4ISNi87"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TIER 1: DQN BACKBONE (Discrete Actions)\n",
        "# ============================================================================\n",
        "\n",
        "def train_dqn_backbone(\n",
        "    config: ExperimentConfig,\n",
        "    seed: int,\n",
        "    experiment_name: str = \"tier1\",\n",
        "    use_dense_reward: bool = None\n",
        ") -> Tuple[Any, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Train DQN backbone on PointMaze with discrete actions.\n",
        "\n",
        "    This is the Tier 1 baseline - a simple DQN without HER.\n",
        "    Expected to struggle with sparse rewards but work with dense rewards.\n",
        "    \"\"\"\n",
        "    set_seeds(seed)\n",
        "\n",
        "    if use_dense_reward is None:\n",
        "        use_dense_reward = config.use_dense_reward\n",
        "\n",
        "    log_dir = os.path.join(config.log_dir, f\"{experiment_name}_dqn_seed{seed}\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Create environments\n",
        "    train_env = make_env(config, seed=seed, use_dense_reward=use_dense_reward)\n",
        "    eval_env = make_env(config, seed=seed + 1000, use_dense_reward=False)  # Eval always sparse\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Training DQN (Tier 1 Backbone)\")\n",
        "    print(f\"Seed: {seed}, Total Steps: {config.total_timesteps}\")\n",
        "    print(f\"Dense Reward: {use_dense_reward}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    print(\"\\n[DEBUG] Checking agent movement...\")\n",
        "    obs, _ = train_env.reset()\n",
        "    for i in range(5):\n",
        "        old_pos = obs['achieved_goal'].copy()\n",
        "        action = train_env.action_space.sample()\n",
        "        obs, r, _, _, info = train_env.step(action)\n",
        "        new_pos = obs['achieved_goal']\n",
        "        dist = np.linalg.norm(new_pos - old_pos)\n",
        "        print(f\"  Step {i}: action={action}, moved={dist:.4f}, reward={r:.4f}\")\n",
        "\n",
        "    # Create DQN model\n",
        "    model = DQN(\n",
        "        \"MultiInputPolicy\",\n",
        "        train_env,\n",
        "        learning_rate=config.learning_rate,\n",
        "        buffer_size=config.buffer_size,\n",
        "        batch_size=config.batch_size,\n",
        "        learning_starts=config.learning_starts,\n",
        "        tau=config.tau,\n",
        "        gamma=config.gamma,\n",
        "        train_freq=config.train_freq,\n",
        "        gradient_steps=config.gradient_steps,\n",
        "        exploration_fraction=config.exploration_fraction,\n",
        "        exploration_initial_eps=config.exploration_initial_eps,\n",
        "        exploration_final_eps=config.exploration_final_eps,\n",
        "        target_update_interval=config.target_update_interval,\n",
        "        policy_kwargs=config.policy_kwargs,\n",
        "        tensorboard_log=log_dir,\n",
        "        verbose=0,\n",
        "        seed=seed,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "    # Create evaluation callback\n",
        "    eval_callback = FixedEvalCallback(\n",
        "        eval_env,\n",
        "        eval_freq=config.eval_freq,\n",
        "        n_eval_episodes=config.n_eval_episodes,\n",
        "        verbose=config.verbose\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "\n",
        "    # FIX: Disable progress_bar to avoid Rich conflicts\n",
        "    model.learn(\n",
        "        total_timesteps=config.total_timesteps,\n",
        "        callback=eval_callback,\n",
        "        progress_bar=False  # FIXED: Avoid Rich recursion bug\n",
        "    )\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    print(f\"\\nTraining completed in {train_time / 60:.1f} minutes\")\n",
        "    print(f\"Best success rate: {eval_callback.best_success_rate:.1%}\")\n",
        "\n",
        "    # Save model\n",
        "    model.save(os.path.join(log_dir, \"final_model\"))\n",
        "\n",
        "    # Get evaluation dataframe\n",
        "    eval_df = eval_callback.get_eval_df()\n",
        "    eval_df['seed'] = seed\n",
        "    eval_df['algorithm'] = 'dqn'\n",
        "    eval_df['use_dense_reward'] = use_dense_reward\n",
        "\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return model, eval_df"
      ],
      "metadata": {
        "id": "Wn0L5kHMNkjb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RUN TIER 1 EXPERIMENTS: DQN Backbone\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"#\" * 60)\n",
        "print(\"# Running Tier 1: DQN Backbone\")\n",
        "print(\"#\" * 60)\n",
        "\n",
        "tier1_results = []\n",
        "\n",
        "for seed in config.seeds:\n",
        "    print(f\"\\n>>> Training DQN with seed {seed}\")\n",
        "    model, df = train_dqn_backbone(config, seed, experiment_name=\"tier1\")\n",
        "    tier1_results.append(df)\n",
        "\n",
        "    # Print progress\n",
        "    if len(df) > 0:\n",
        "        final_success = df['success_rate'].iloc[-1]\n",
        "        print(f\"    Final success rate: {final_success:.1%}\")\n",
        "\n",
        "tier1_df = pd.concat(tier1_results, ignore_index=True) if tier1_results else pd.DataFrame()\n",
        "print(\"\\nâœ“ Tier 1 complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHJfOpeQNl87",
        "outputId": "a33df809-35ce-40d9-cbcf-6e3354457dd5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# Running Tier 1: DQN Backbone\n",
            "############################################################\n",
            "\n",
            ">>> Training DQN with seed 42\n",
            "\n",
            "============================================================\n",
            "Training DQN (Tier 1 Backbone)\n",
            "Seed: 42, Total Steps: 750000\n",
            "Dense Reward: True\n",
            "============================================================\n",
            "\n",
            "[DEBUG] Checking agent movement...\n",
            "  Step 0: action=0, moved=0.0000, reward=0.0000\n",
            "  Step 1: action=0, moved=0.0000, reward=0.0000\n",
            "  Step 2: action=1, moved=0.0356, reward=-0.0177\n",
            "  Step 3: action=3, moved=0.0688, reward=-0.0606\n",
            "  Step 4: action=0, moved=0.0827, reward=-0.0804\n",
            "\n",
            "Training completed in 23.0 minutes\n",
            "Best success rate: 0.0%\n",
            "    Final success rate: 0.0%\n",
            "\n",
            "âœ“ Tier 1 complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TIER 2: DQN + HER (Hindsight Experience Replay)\n",
        "# ============================================================================\n",
        "\n",
        "def train_dqn_her(\n",
        "    config: ExperimentConfig,\n",
        "    seed: int,\n",
        "    use_her: bool = True,\n",
        "    experiment_name: str = \"tier2\"\n",
        ") -> Tuple[Any, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Train DQN with or without HER.\n",
        "\n",
        "    This is the key ablation:\n",
        "    - WITHOUT HER: Should struggle with sparse rewards\n",
        "    - WITH HER: Should learn effectively by relabeling failed trajectories\n",
        "    \"\"\"\n",
        "    set_seeds(seed)\n",
        "\n",
        "    her_str = \"with_her\" if use_her else \"no_her\"\n",
        "    log_dir = os.path.join(config.log_dir, f\"{experiment_name}_dqn_{her_str}_seed{seed}\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Create environments (SPARSE rewards for proper HER ablation)\n",
        "    train_env = make_env(config, seed=seed, use_dense_reward=False)\n",
        "    eval_env = make_env(config, seed=seed + 1000, use_dense_reward=False)\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Training DQN {'WITH' if use_her else 'WITHOUT'} HER (Tier 2)\")\n",
        "    print(f\"Seed: {seed}, Total Steps: {config.total_timesteps}\")\n",
        "    print(f\"Reward: SPARSE (for proper ablation)\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    # Configure replay buffer\n",
        "    if use_her:\n",
        "        replay_buffer_class = HerReplayBuffer\n",
        "        replay_buffer_kwargs = {\n",
        "            \"n_sampled_goal\": config.her_n_sampled_goal,\n",
        "            \"goal_selection_strategy\": config.her_strategy,\n",
        "        }\n",
        "    else:\n",
        "        replay_buffer_class = DictReplayBuffer\n",
        "        replay_buffer_kwargs = {}\n",
        "\n",
        "    # Create model\n",
        "    model = DQN(\n",
        "        \"MultiInputPolicy\",\n",
        "        train_env,\n",
        "        learning_rate=config.learning_rate,\n",
        "        buffer_size=config.buffer_size,\n",
        "        batch_size=config.batch_size,\n",
        "        learning_starts=config.learning_starts,\n",
        "        tau=config.tau,\n",
        "        gamma=config.gamma,\n",
        "        train_freq=config.train_freq,\n",
        "        gradient_steps=config.gradient_steps,\n",
        "        exploration_fraction=config.exploration_fraction,\n",
        "        exploration_initial_eps=config.exploration_initial_eps,\n",
        "        exploration_final_eps=config.exploration_final_eps,\n",
        "        target_update_interval=config.target_update_interval,\n",
        "        policy_kwargs=config.policy_kwargs,\n",
        "        replay_buffer_class=replay_buffer_class,\n",
        "        replay_buffer_kwargs=replay_buffer_kwargs,\n",
        "        tensorboard_log=log_dir,\n",
        "        verbose=0,\n",
        "        seed=seed,\n",
        "        device=DEVICE,\n",
        "    )\n",
        "\n",
        "    # Create evaluation callback\n",
        "    eval_callback = FixedEvalCallback(\n",
        "        eval_env,\n",
        "        eval_freq=config.eval_freq,\n",
        "        n_eval_episodes=config.n_eval_episodes,\n",
        "        verbose=config.verbose\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    model.learn(\n",
        "        total_timesteps=config.total_timesteps,\n",
        "        callback=eval_callback,\n",
        "        progress_bar=False\n",
        "    )\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    print(f\"\\nTraining completed in {train_time / 60:.1f} minutes\")\n",
        "    print(f\"Best success rate: {eval_callback.best_success_rate:.1%}\")\n",
        "\n",
        "    # Save\n",
        "    model.save(os.path.join(log_dir, \"final_model\"))\n",
        "\n",
        "    eval_df = eval_callback.get_eval_df()\n",
        "    eval_df['seed'] = seed\n",
        "    eval_df['algorithm'] = f\"dqn_{her_str}\"\n",
        "    eval_df['use_her'] = use_her\n",
        "\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return model, eval_df"
      ],
      "metadata": {
        "id": "-Jl58n0yNnXT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RUN TIER 2 EXPERIMENTS: HER ABLATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"#\" * 60)\n",
        "print(\"# Running Tier 2: HER Ablation\")\n",
        "print(\"#\" * 60)\n",
        "\n",
        "tier2_results = []\n",
        "\n",
        "# Train WITHOUT HER (expected to struggle)\n",
        "for seed in config.seeds:\n",
        "    print(f\"\\n>>> Training DQN WITHOUT HER (seed {seed})\")\n",
        "    model, df = train_dqn_her(config, seed, use_her=False, experiment_name=\"tier2\")\n",
        "    tier2_results.append(df)\n",
        "    if len(df) > 0:\n",
        "        print(f\"    Final success rate: {df['success_rate'].iloc[-1]:.1%}\")\n",
        "\n",
        "# Train WITH HER (expected to learn)\n",
        "for seed in config.seeds:\n",
        "    print(f\"\\n>>> Training DQN WITH HER (seed {seed})\")\n",
        "    model, df = train_dqn_her(config, seed, use_her=True, experiment_name=\"tier2\")\n",
        "    tier2_results.append(df)\n",
        "    if len(df) > 0:\n",
        "        print(f\"    Final success rate: {df['success_rate'].iloc[-1]:.1%}\")\n",
        "\n",
        "tier2_df = pd.concat(tier2_results, ignore_index=True) if tier2_results else pd.DataFrame()\n",
        "print(\"\\nâœ“ Tier 2 complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1DExbwENo2z",
        "outputId": "7e2cc1c4-49f4-47dc-b847-5b656f8360ca"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# Running Tier 2: HER Ablation\n",
            "############################################################\n",
            "\n",
            ">>> Training DQN WITHOUT HER (seed 42)\n",
            "\n",
            "============================================================\n",
            "Training DQN WITHOUT HER (Tier 2)\n",
            "Seed: 42, Total Steps: 750000\n",
            "Reward: SPARSE (for proper ablation)\n",
            "============================================================\n",
            "\n",
            "Training completed in 23.0 minutes\n",
            "Best success rate: 0.0%\n",
            "    Final success rate: 0.0%\n",
            "\n",
            ">>> Training DQN WITH HER (seed 42)\n",
            "\n",
            "============================================================\n",
            "Training DQN WITH HER (Tier 2)\n",
            "Seed: 42, Total Steps: 750000\n",
            "Reward: SPARSE (for proper ablation)\n",
            "============================================================\n",
            "\n",
            "Training completed in 28.3 minutes\n",
            "Best success rate: 0.0%\n",
            "    Final success rate: 0.0%\n",
            "\n",
            "âœ“ Tier 2 complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TIER 3: HAC (Hierarchical Actor-Critic) IMPLEMENTATION\n",
        "# ============================================================================\n",
        "\n",
        "class HACReplayBuffer:\n",
        "    \"\"\"\n",
        "    Hierarchical replay buffer for HAC.\n",
        "\n",
        "    Stores transitions for both high-level (subgoal selection) and\n",
        "    low-level (primitive action) policies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        buffer_size: int,\n",
        "        obs_dim: int,\n",
        "        goal_dim: int,\n",
        "        subgoal_dim: int,\n",
        "        n_actions: int,  # Discrete action count\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.device = device\n",
        "\n",
        "        # High-level buffer (continuous subgoals)\n",
        "        self.high_buffer = {\n",
        "            'obs': np.zeros((buffer_size, obs_dim), dtype=np.float32),\n",
        "            'goal': np.zeros((buffer_size, goal_dim), dtype=np.float32),\n",
        "            'subgoal': np.zeros((buffer_size, subgoal_dim), dtype=np.float32),\n",
        "            'reward': np.zeros((buffer_size, 1), dtype=np.float32),\n",
        "            'next_obs': np.zeros((buffer_size, obs_dim), dtype=np.float32),\n",
        "            'done': np.zeros((buffer_size, 1), dtype=np.float32),\n",
        "        }\n",
        "        self.high_ptr = 0\n",
        "        self.high_size = 0\n",
        "\n",
        "        # Low-level buffer (discrete actions)\n",
        "        self.low_buffer = {\n",
        "            'obs': np.zeros((buffer_size, obs_dim), dtype=np.float32),\n",
        "            'subgoal': np.zeros((buffer_size, subgoal_dim), dtype=np.float32),\n",
        "            'action': np.zeros((buffer_size,), dtype=np.int64),  # Discrete!\n",
        "            'reward': np.zeros((buffer_size, 1), dtype=np.float32),\n",
        "            'next_obs': np.zeros((buffer_size, obs_dim), dtype=np.float32),\n",
        "            'done': np.zeros((buffer_size, 1), dtype=np.float32),\n",
        "        }\n",
        "        self.low_ptr = 0\n",
        "        self.low_size = 0\n",
        "\n",
        "    def add_high_transition(self, obs, goal, subgoal, reward, next_obs, done):\n",
        "        idx = self.high_ptr\n",
        "        self.high_buffer['obs'][idx] = obs\n",
        "        self.high_buffer['goal'][idx] = goal\n",
        "        self.high_buffer['subgoal'][idx] = subgoal\n",
        "        self.high_buffer['reward'][idx] = reward\n",
        "        self.high_buffer['next_obs'][idx] = next_obs\n",
        "        self.high_buffer['done'][idx] = done\n",
        "        self.high_ptr = (self.high_ptr + 1) % self.buffer_size\n",
        "        self.high_size = min(self.high_size + 1, self.buffer_size)\n",
        "\n",
        "    def add_low_transition(self, obs, subgoal, action, reward, next_obs, done):\n",
        "        idx = self.low_ptr\n",
        "        self.low_buffer['obs'][idx] = obs\n",
        "        self.low_buffer['subgoal'][idx] = subgoal\n",
        "        self.low_buffer['action'][idx] = action\n",
        "        self.low_buffer['reward'][idx] = reward\n",
        "        self.low_buffer['next_obs'][idx] = next_obs\n",
        "        self.low_buffer['done'][idx] = done\n",
        "        self.low_ptr = (self.low_ptr + 1) % self.buffer_size\n",
        "        self.low_size = min(self.low_size + 1, self.buffer_size)\n",
        "\n",
        "    def sample_high(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        indices = np.random.randint(0, self.high_size, size=batch_size)\n",
        "        return {k: torch.FloatTensor(v[indices]).to(self.device)\n",
        "                for k, v in self.high_buffer.items()}\n",
        "\n",
        "    def sample_low(self, batch_size: int) -> Dict[str, torch.Tensor]:\n",
        "        indices = np.random.randint(0, self.low_size, size=batch_size)\n",
        "        batch = {}\n",
        "        for k, v in self.low_buffer.items():\n",
        "            if k == 'action':\n",
        "                batch[k] = torch.LongTensor(v[indices]).to(self.device)\n",
        "            else:\n",
        "                batch[k] = torch.FloatTensor(v[indices]).to(self.device)\n",
        "        return batch\n",
        "\n",
        "\n",
        "class HighLevelPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    High-level policy that outputs continuous subgoal positions.\n",
        "    Uses SAC-style actor-critic for continuous subgoal output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        goal_dim: int,\n",
        "        subgoal_dim: int,\n",
        "        hidden_dims: List[int] = [256, 256],\n",
        "        subgoal_range: Tuple[float, float] = (0.0, 8.0),\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.subgoal_dim = subgoal_dim\n",
        "        self.subgoal_low = subgoal_range[0]\n",
        "        self.subgoal_high = subgoal_range[1]\n",
        "        self.device = device\n",
        "\n",
        "        input_dim = obs_dim + goal_dim\n",
        "\n",
        "        # Actor network (outputs subgoal)\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.extend([nn.Linear(prev_dim, h), nn.ReLU()])\n",
        "            prev_dim = h\n",
        "        self.actor_net = nn.Sequential(*layers)\n",
        "        self.mean_head = nn.Linear(prev_dim, subgoal_dim)\n",
        "        self.log_std_head = nn.Linear(prev_dim, subgoal_dim)\n",
        "\n",
        "        # Critic networks\n",
        "        critic_input = input_dim + subgoal_dim\n",
        "\n",
        "        def make_critic():\n",
        "            layers = []\n",
        "            prev = critic_input\n",
        "            for h in hidden_dims:\n",
        "                layers.extend([nn.Linear(prev, h), nn.ReLU()])\n",
        "                prev = h\n",
        "            layers.append(nn.Linear(prev, 1))\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.q1 = make_critic()\n",
        "        self.q2 = make_critic()\n",
        "        self.q1_target = deepcopy(self.q1)\n",
        "        self.q2_target = deepcopy(self.q2)\n",
        "\n",
        "        for p in self.q1_target.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.q2_target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.log_alpha = nn.Parameter(torch.zeros(1))\n",
        "        self.target_entropy = -subgoal_dim\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def get_subgoal(self, obs: np.ndarray, goal: np.ndarray, deterministic: bool = False) -> np.ndarray:\n",
        "        with torch.no_grad():\n",
        "            obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            goal_t = torch.FloatTensor(goal).unsqueeze(0).to(self.device)\n",
        "            x = torch.cat([obs_t, goal_t], dim=-1)\n",
        "\n",
        "            features = self.actor_net(x)\n",
        "            mean = self.mean_head(features)\n",
        "\n",
        "            if deterministic:\n",
        "                subgoal = torch.sigmoid(mean)\n",
        "            else:\n",
        "                log_std = torch.clamp(self.log_std_head(features), -20, 2)\n",
        "                std = log_std.exp()\n",
        "                dist = torch.distributions.Normal(mean, std)\n",
        "                subgoal = torch.sigmoid(dist.rsample())\n",
        "\n",
        "            # Scale to subgoal range\n",
        "            subgoal = subgoal * (self.subgoal_high - self.subgoal_low) + self.subgoal_low\n",
        "            return subgoal.cpu().numpy().flatten()\n",
        "\n",
        "    def forward_actor(self, obs: torch.Tensor, goal: torch.Tensor):\n",
        "        x = torch.cat([obs, goal], dim=-1)\n",
        "        features = self.actor_net(x)\n",
        "        mean = self.mean_head(features)\n",
        "        log_std = torch.clamp(self.log_std_head(features), -20, 2)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        z = dist.rsample()\n",
        "        subgoal = torch.sigmoid(z)\n",
        "\n",
        "        # Log prob with sigmoid correction\n",
        "        log_prob = dist.log_prob(z) - torch.log(subgoal * (1 - subgoal) + 1e-6)\n",
        "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        subgoal = subgoal * (self.subgoal_high - self.subgoal_low) + self.subgoal_low\n",
        "        return subgoal, log_prob\n",
        "\n",
        "    def forward_critic(self, obs, goal, subgoal):\n",
        "        x = torch.cat([obs, goal, subgoal], dim=-1)\n",
        "        return self.q1(x), self.q2(x)\n",
        "\n",
        "    def forward_critic_target(self, obs, goal, subgoal):\n",
        "        x = torch.cat([obs, goal, subgoal], dim=-1)\n",
        "        return self.q1_target(x), self.q2_target(x)\n",
        "\n",
        "    def soft_update(self, tau: float = 0.005):\n",
        "        for tp, p in zip(self.q1_target.parameters(), self.q1.parameters()):\n",
        "            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)\n",
        "        for tp, p in zip(self.q2_target.parameters(), self.q2.parameters()):\n",
        "            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)\n",
        "\n",
        "\n",
        "class LowLevelPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Low-level DQN policy that outputs discrete actions to reach subgoals.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        subgoal_dim: int,\n",
        "        n_actions: int,\n",
        "        hidden_dims: List[int] = [256, 256],\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.device = device\n",
        "\n",
        "        input_dim = obs_dim + subgoal_dim\n",
        "\n",
        "        # Q-network\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.extend([nn.Linear(prev_dim, h), nn.ReLU()])\n",
        "            prev_dim = h\n",
        "        layers.append(nn.Linear(prev_dim, n_actions))\n",
        "        self.q_net = nn.Sequential(*layers)\n",
        "\n",
        "        # Target network\n",
        "        self.q_target = deepcopy(self.q_net)\n",
        "        for p in self.q_target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def get_action(self, obs: np.ndarray, subgoal: np.ndarray,\n",
        "                   epsilon: float = 0.0, deterministic: bool = False) -> int:\n",
        "        if not deterministic and np.random.random() < epsilon:\n",
        "            return np.random.randint(self.n_actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
        "            subgoal_t = torch.FloatTensor(subgoal).unsqueeze(0).to(self.device)\n",
        "            x = torch.cat([obs_t, subgoal_t], dim=-1)\n",
        "            q_values = self.q_net(x)\n",
        "            return q_values.argmax(dim=-1).item()\n",
        "\n",
        "    def forward(self, obs: torch.Tensor, subgoal: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.cat([obs, subgoal], dim=-1)\n",
        "        return self.q_net(x)\n",
        "\n",
        "    def forward_target(self, obs: torch.Tensor, subgoal: torch.Tensor) -> torch.Tensor:\n",
        "        x = torch.cat([obs, subgoal], dim=-1)\n",
        "        return self.q_target(x)\n",
        "\n",
        "    def soft_update(self, tau: float = 0.005):\n",
        "        for tp, p in zip(self.q_target.parameters(), self.q_net.parameters()):\n",
        "            tp.data.copy_(tau * p.data + (1 - tau) * tp.data)\n",
        "\n",
        "\n",
        "class HACAgent:\n",
        "    \"\"\"\n",
        "    Hierarchical Actor-Critic (HAC) Agent.\n",
        "\n",
        "    High-level: SAC-style policy outputs continuous subgoal positions\n",
        "    Low-level: DQN policy outputs discrete actions to reach subgoals\n",
        "\n",
        "    FIXES from original:\n",
        "    1. Properly tracks state when subgoal is issued\n",
        "    2. Correct hindsight relabeling for both levels\n",
        "    3. Proper episode boundary handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_dim: int,\n",
        "        goal_dim: int,\n",
        "        n_actions: int,\n",
        "        subgoal_dim: int,\n",
        "        config: ExperimentConfig,\n",
        "        device: str = \"cpu\"\n",
        "    ):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.subgoal_dim = subgoal_dim\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        self.subgoal_period = config.subgoal_period_k\n",
        "\n",
        "        # Create policies\n",
        "        self.high_policy = HighLevelPolicy(\n",
        "            obs_dim=obs_dim,\n",
        "            goal_dim=goal_dim,\n",
        "            subgoal_dim=subgoal_dim,\n",
        "            subgoal_range=config.subgoal_range,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        self.low_policy = LowLevelPolicy(\n",
        "            obs_dim=obs_dim,\n",
        "            subgoal_dim=subgoal_dim,\n",
        "            n_actions=n_actions,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Optimizers\n",
        "        self.high_actor_optim = Adam(\n",
        "            list(self.high_policy.actor_net.parameters()) +\n",
        "            list(self.high_policy.mean_head.parameters()) +\n",
        "            list(self.high_policy.log_std_head.parameters()),\n",
        "            lr=config.high_level_lr\n",
        "        )\n",
        "        self.high_critic_optim = Adam(\n",
        "            list(self.high_policy.q1.parameters()) +\n",
        "            list(self.high_policy.q2.parameters()),\n",
        "            lr=config.high_level_lr\n",
        "        )\n",
        "        self.high_alpha_optim = Adam([self.high_policy.log_alpha], lr=config.high_level_lr)\n",
        "\n",
        "        self.low_optim = Adam(self.low_policy.q_net.parameters(), lr=config.low_level_lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.buffer = HACReplayBuffer(\n",
        "            buffer_size=config.buffer_size,\n",
        "            obs_dim=obs_dim,\n",
        "            goal_dim=goal_dim,\n",
        "            subgoal_dim=subgoal_dim,\n",
        "            n_actions=n_actions,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Episode tracking\n",
        "        self.current_subgoal = None\n",
        "        self.steps_since_subgoal = 0\n",
        "        self.high_start_state = None\n",
        "        self.high_start_goal = None\n",
        "        self.total_steps = 0\n",
        "\n",
        "        # Exploration\n",
        "        self.epsilon = config.exploration_initial_eps\n",
        "        self.epsilon_decay = (config.exploration_initial_eps - config.exploration_final_eps) / \\\n",
        "                            (config.total_timesteps * config.exploration_fraction)\n",
        "        self.epsilon_min = config.exploration_final_eps\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset agent state at episode start.\"\"\"\n",
        "        self.current_subgoal = None\n",
        "        self.steps_since_subgoal = 0\n",
        "        self.high_start_state = None\n",
        "        self.high_start_goal = None\n",
        "\n",
        "    def act(self, obs: Dict[str, np.ndarray], deterministic: bool = False) -> Tuple[int, np.ndarray]:\n",
        "        \"\"\"Select action using hierarchical policy.\"\"\"\n",
        "        state = obs['observation']\n",
        "        final_goal = obs['desired_goal']\n",
        "\n",
        "        # Check if we need new subgoal\n",
        "        if self.current_subgoal is None or self.steps_since_subgoal >= self.subgoal_period:\n",
        "            self.high_start_state = state.copy()\n",
        "            self.high_start_goal = final_goal.copy()\n",
        "            self.current_subgoal = self.high_policy.get_subgoal(state, final_goal, deterministic)\n",
        "            self.steps_since_subgoal = 0\n",
        "\n",
        "        # Get discrete action from low-level policy\n",
        "        eps = 0.0 if deterministic else self.epsilon\n",
        "        action = self.low_policy.get_action(state, self.current_subgoal, eps, deterministic)\n",
        "        self.steps_since_subgoal += 1\n",
        "\n",
        "        return action, self.current_subgoal.copy()\n",
        "\n",
        "    def compute_low_reward(self, achieved: np.ndarray, subgoal: np.ndarray, threshold: float = 0.5) -> float:\n",
        "        \"\"\"Reward for low-level: distance to subgoal.\"\"\"\n",
        "        distance = np.linalg.norm(achieved[:self.subgoal_dim] - subgoal)\n",
        "        return 0.0 if distance < threshold else -1.0\n",
        "\n",
        "    def compute_high_reward(self, achieved: np.ndarray, final_goal: np.ndarray, threshold: float = 0.45) -> float:\n",
        "        \"\"\"Reward for high-level: distance to final goal.\"\"\"\n",
        "        distance = np.linalg.norm(achieved[:self.goal_dim] - final_goal)\n",
        "        return 0.0 if distance < threshold else -1.0\n",
        "\n",
        "    def store_transition(self, obs, action, subgoal, next_obs, done, info):\n",
        "        \"\"\"Store transition and handle hindsight.\"\"\"\n",
        "        state = obs['observation']\n",
        "        achieved = obs['achieved_goal']\n",
        "        final_goal = obs['desired_goal']\n",
        "        next_state = next_obs['observation']\n",
        "        next_achieved = next_obs['achieved_goal']\n",
        "\n",
        "        # Low-level transition\n",
        "        low_reward = self.compute_low_reward(next_achieved, subgoal)\n",
        "        self.buffer.add_low_transition(state, subgoal, action, low_reward, next_state, done)\n",
        "\n",
        "        # High-level transition with HER\n",
        "        if self.steps_since_subgoal >= self.subgoal_period or done:\n",
        "            if self.high_start_state is not None:\n",
        "                # Original transition\n",
        "                high_reward = self.compute_high_reward(next_achieved, final_goal)\n",
        "                self.buffer.add_high_transition(\n",
        "                    self.high_start_state, self.high_start_goal, subgoal,\n",
        "                    high_reward, next_state, done\n",
        "                )\n",
        "\n",
        "                # HINDSIGHT: Relabel with achieved position as goal\n",
        "                hindsight_goal = next_achieved.copy()\n",
        "                hindsight_reward = 0.0  # Success by definition\n",
        "                self.buffer.add_high_transition(\n",
        "                    self.high_start_state, hindsight_goal, subgoal,\n",
        "                    hindsight_reward, next_state, done\n",
        "                )\n",
        "\n",
        "        # Update exploration\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon - self.epsilon_decay)\n",
        "        self.total_steps += 1\n",
        "\n",
        "    def train_step(self) -> Dict[str, float]:\n",
        "        \"\"\"Perform one training step for both levels.\"\"\"\n",
        "        losses = {}\n",
        "        batch_size = self.config.batch_size\n",
        "\n",
        "        # Train high-level (SAC)\n",
        "        if self.buffer.high_size >= batch_size:\n",
        "            batch = self.buffer.sample_high(batch_size)\n",
        "            high_loss = self._update_high_level(batch)\n",
        "            losses.update({f'high_{k}': v for k, v in high_loss.items()})\n",
        "\n",
        "        # Train low-level (DQN)\n",
        "        if self.buffer.low_size >= batch_size:\n",
        "            batch = self.buffer.sample_low(batch_size)\n",
        "            low_loss = self._update_low_level(batch)\n",
        "            losses.update({f'low_{k}': v for k, v in low_loss.items()})\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _update_high_level(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        \"\"\"Update high-level SAC policy.\"\"\"\n",
        "        obs = batch['obs']\n",
        "        goal = batch['goal']\n",
        "        subgoal = batch['subgoal']\n",
        "        reward = batch['reward']\n",
        "        next_obs = batch['next_obs']\n",
        "        done = batch['done']\n",
        "\n",
        "        gamma = self.config.gamma ** self.subgoal_period\n",
        "\n",
        "        # Update critics\n",
        "        with torch.no_grad():\n",
        "            next_subgoal, next_log_prob = self.high_policy.forward_actor(next_obs, goal)\n",
        "            q1_target, q2_target = self.high_policy.forward_critic_target(next_obs, goal, next_subgoal)\n",
        "            q_target = torch.min(q1_target, q2_target)\n",
        "            alpha = self.high_policy.log_alpha.exp()\n",
        "            target = reward + gamma * (1 - done) * (q_target - alpha * next_log_prob)\n",
        "\n",
        "        q1, q2 = self.high_policy.forward_critic(obs, goal, subgoal)\n",
        "        critic_loss = F.mse_loss(q1, target) + F.mse_loss(q2, target)\n",
        "\n",
        "        self.high_critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.high_critic_optim.step()\n",
        "\n",
        "        # Update actor\n",
        "        new_subgoal, log_prob = self.high_policy.forward_actor(obs, goal)\n",
        "        q1_new, q2_new = self.high_policy.forward_critic(obs, goal, new_subgoal)\n",
        "        q_new = torch.min(q1_new, q2_new)\n",
        "\n",
        "        alpha = self.high_policy.log_alpha.exp().detach()\n",
        "        actor_loss = (alpha * log_prob - q_new).mean()\n",
        "\n",
        "        self.high_actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.high_actor_optim.step()\n",
        "\n",
        "        # Update alpha\n",
        "        alpha_loss = -(self.high_policy.log_alpha *\n",
        "                      (log_prob.detach() + self.high_policy.target_entropy)).mean()\n",
        "\n",
        "        self.high_alpha_optim.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.high_alpha_optim.step()\n",
        "\n",
        "        self.high_policy.soft_update(self.config.tau)\n",
        "\n",
        "        return {'critic_loss': critic_loss.item(), 'actor_loss': actor_loss.item()}\n",
        "\n",
        "    def _update_low_level(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        \"\"\"Update low-level DQN policy.\"\"\"\n",
        "        obs = batch['obs']\n",
        "        subgoal = batch['subgoal']\n",
        "        action = batch['action']\n",
        "        reward = batch['reward']\n",
        "        next_obs = batch['next_obs']\n",
        "        done = batch['done']\n",
        "\n",
        "        # Current Q values\n",
        "        q_values = self.low_policy(obs, subgoal)\n",
        "        q_selected = q_values.gather(1, action.unsqueeze(1))\n",
        "\n",
        "        # Target Q values (Double DQN style)\n",
        "        with torch.no_grad():\n",
        "            next_q = self.low_policy(next_obs, subgoal)\n",
        "            next_action = next_q.argmax(dim=1, keepdim=True)\n",
        "            next_q_target = self.low_policy.forward_target(next_obs, subgoal)\n",
        "            next_q_selected = next_q_target.gather(1, next_action)\n",
        "            target = reward + self.config.gamma * (1 - done) * next_q_selected\n",
        "\n",
        "        loss = F.mse_loss(q_selected, target)\n",
        "\n",
        "        self.low_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.low_optim.step()\n",
        "\n",
        "        self.low_policy.soft_update(self.config.tau)\n",
        "\n",
        "        return {'q_loss': loss.item()}"
      ],
      "metadata": {
        "id": "fE09oIzlNqNc"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# HAC TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_hac(agent: HACAgent, env: gym.Env, n_episodes: int) -> Dict[str, float]:\n",
        "    \"\"\"Evaluate HAC agent.\"\"\"\n",
        "    successes = []\n",
        "    steps_list = []\n",
        "    path_lengths = []\n",
        "    goal_distances = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        agent.reset()\n",
        "\n",
        "        start_pos = obs['achieved_goal'].copy()\n",
        "        goal_pos = obs['desired_goal'].copy()\n",
        "        goal_distance = np.linalg.norm(goal_pos - start_pos)\n",
        "\n",
        "        path_length = 0.0\n",
        "        prev_pos = start_pos.copy()\n",
        "        steps = 0\n",
        "        done = False\n",
        "        success = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = agent.act(obs, deterministic=True)\n",
        "            obs, _, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            current_pos = obs['achieved_goal']\n",
        "            path_length += np.linalg.norm(current_pos - prev_pos)\n",
        "            prev_pos = current_pos.copy()\n",
        "            steps += 1\n",
        "\n",
        "            if info.get('is_success', False):\n",
        "                success = True\n",
        "\n",
        "        successes.append(float(success))\n",
        "        steps_list.append(steps)\n",
        "        path_lengths.append(path_length)\n",
        "        goal_distances.append(goal_distance)\n",
        "\n",
        "    efficiencies = [gd / pl if pl > 0 else 0 for gd, pl in zip(goal_distances, path_lengths)]\n",
        "    successful_steps = [s for s, succ in zip(steps_list, successes) if succ]\n",
        "\n",
        "    return {\n",
        "        'success_rate': np.mean(successes),\n",
        "        'success_rate_std': np.std(successes),\n",
        "        'mean_steps': np.mean(steps_list),\n",
        "        'std_steps': np.std(steps_list),\n",
        "        'mean_steps_successful': np.mean(successful_steps) if successful_steps else float('inf'),\n",
        "        'mean_path_efficiency': np.mean(efficiencies),\n",
        "        'std_path_efficiency': np.std(efficiencies),\n",
        "    }\n",
        "\n",
        "\n",
        "def train_hac(config: ExperimentConfig, seed: int, experiment_name: str = \"tier3\") -> Tuple[HACAgent, pd.DataFrame]:\n",
        "    \"\"\"Train HAC agent.\"\"\"\n",
        "    set_seeds(seed)\n",
        "\n",
        "    log_dir = os.path.join(config.log_dir, f\"{experiment_name}_hac_seed{seed}\")\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Create environments\n",
        "    train_env = make_env(config, seed=seed, use_dense_reward=False)\n",
        "    eval_env = make_env(config, seed=seed + 1000, use_dense_reward=False)\n",
        "\n",
        "    # Get dimensions\n",
        "    obs, _ = train_env.reset()\n",
        "    obs_dim = obs['observation'].shape[0]\n",
        "    goal_dim = obs['desired_goal'].shape[0]\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"Training HAC (Tier 3 - Hierarchical RL)\")\n",
        "    print(f\"Seed: {seed}, Total Steps: {config.total_timesteps}\")\n",
        "    print(f\"Subgoal Period K: {config.subgoal_period_k}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    # Create agent\n",
        "    agent = HACAgent(\n",
        "        obs_dim=obs_dim,\n",
        "        goal_dim=goal_dim,\n",
        "        n_actions=config.n_discrete_actions,\n",
        "        subgoal_dim=config.subgoal_dim,\n",
        "        config=config,\n",
        "        device=str(DEVICE)\n",
        "    )\n",
        "\n",
        "    eval_history = []\n",
        "    total_steps = 0\n",
        "    best_success_rate = 0.0\n",
        "\n",
        "    pbar = tqdm(total=config.total_timesteps, desc=\"Training HAC\")\n",
        "\n",
        "    while total_steps < config.total_timesteps:\n",
        "        obs, _ = train_env.reset()\n",
        "        agent.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done and total_steps < config.total_timesteps:\n",
        "            action, subgoal = agent.act(obs)\n",
        "            next_obs, _, terminated, truncated, info = train_env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.store_transition(obs, action, subgoal, next_obs, done, info)\n",
        "\n",
        "            obs = next_obs\n",
        "            total_steps += 1\n",
        "\n",
        "            # Train\n",
        "            if total_steps > config.learning_starts:\n",
        "                agent.train_step()\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Evaluate\n",
        "            if total_steps % config.eval_freq == 0:\n",
        "                metrics = evaluate_hac(agent, eval_env, config.n_eval_episodes)\n",
        "                metrics['timestep'] = total_steps\n",
        "                eval_history.append(metrics)\n",
        "\n",
        "                if metrics['success_rate'] > best_success_rate:\n",
        "                    best_success_rate = metrics['success_rate']\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'success': f\"{metrics['success_rate']:.1%}\",\n",
        "                    'best': f\"{best_success_rate:.1%}\"\n",
        "                })\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "    print(f\"\\nTraining completed!\")\n",
        "    print(f\"Best success rate: {best_success_rate:.1%}\")\n",
        "\n",
        "    eval_df = pd.DataFrame(eval_history)\n",
        "    eval_df['seed'] = seed\n",
        "    eval_df['algorithm'] = 'hac'\n",
        "\n",
        "    train_env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "    return agent, eval_df"
      ],
      "metadata": {
        "id": "j31SHJk-Nr5r"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# RUN TIER 3 EXPERIMENTS: HAC\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"#\" * 60)\n",
        "print(\"# Running Tier 3: HAC (Hierarchical RL)\")\n",
        "print(\"#\" * 60)\n",
        "\n",
        "tier3_results = []\n",
        "\n",
        "for seed in config.seeds:\n",
        "    print(f\"\\n>>> Training HAC with seed {seed}\")\n",
        "    agent, df = train_hac(config, seed, experiment_name=\"tier3\")\n",
        "    tier3_results.append(df)\n",
        "    if len(df) > 0:\n",
        "        print(f\"    Final success rate: {df['success_rate'].iloc[-1]:.1%}\")\n",
        "\n",
        "tier3_df = pd.concat(tier3_results, ignore_index=True) if tier3_results else pd.DataFrame()\n",
        "print(\"\\nâœ“ Tier 3 complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "241fafe85fc84b5c8a2b1dbfbde1a2b2",
            "e8fe2b2c476244a7b6d18fe0444abc24",
            "83c940ba65784bed92fe5425e9b4ac7f",
            "6f53d27633da413ea7f861cdf240a6fd",
            "4ee7adb6e44b454490776c17aa6ff088",
            "204d4eee7a58495f882dbf47fa986912",
            "fbec180237184504883e6d18089616e5",
            "802754f00585489baf2777d931cd811f",
            "bbe3b64c735e4424b43b6469b7826fe8",
            "5e6e044b69cd4fe0b6182f578c3cd97e",
            "50effa7ffb124c46a022e8ea2c7485e1"
          ]
        },
        "id": "QJCnMc9WNtTD",
        "outputId": "d11a8795-d885-4476-b41e-3037432fb00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "############################################################\n",
            "# Running Tier 3: HAC (Hierarchical RL)\n",
            "############################################################\n",
            "\n",
            ">>> Training HAC with seed 42\n",
            "\n",
            "============================================================\n",
            "Training HAC (Tier 3 - Hierarchical RL)\n",
            "Seed: 42, Total Steps: 750000\n",
            "Subgoal Period K: 30\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training HAC:   0%|          | 0/750000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "241fafe85fc84b5c8a2b1dbfbde1a2b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION: Learning Curves and Comparisons\n",
        "# ============================================================================\n",
        "\n",
        "def plot_learning_curves(dataframes: Dict[str, pd.DataFrame], title: str, save_path: str = None):\n",
        "    \"\"\"Plot learning curves with confidence intervals.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    colors = plt.cm.Set2.colors\n",
        "\n",
        "    for i, (name, df) in enumerate(dataframes.items()):\n",
        "        if df is None or len(df) == 0:\n",
        "            continue\n",
        "\n",
        "        # Aggregate across seeds\n",
        "        agg = df.groupby('timestep')['success_rate'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "        color = colors[i % len(colors)]\n",
        "        ax.plot(agg['timestep'], agg['mean'], label=name, color=color, linewidth=2)\n",
        "        ax.fill_between(\n",
        "            agg['timestep'],\n",
        "            agg['mean'] - agg['std'],\n",
        "            agg['mean'] + agg['std'],\n",
        "            color=color,\n",
        "            alpha=0.2\n",
        "        )\n",
        "\n",
        "    ax.set_xlabel('Training Steps', fontsize=12)\n",
        "    ax.set_ylabel('Success Rate', fontsize=12)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\n",
        "    ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot Tier 1 results\n",
        "if len(tier1_df) > 0:\n",
        "    plot_learning_curves(\n",
        "        {'DQN (Dense Reward)': tier1_df},\n",
        "        'Tier 1: DQN Backbone Learning Curve',\n",
        "        os.path.join(config.log_dir, 'tier1_learning_curve.png')\n",
        "    )\n",
        "\n",
        "# Plot HER Ablation (Tier 2)\n",
        "if len(tier2_df) > 0:\n",
        "    her_data = {}\n",
        "    for use_her in [True, False]:\n",
        "        subset = tier2_df[tier2_df['use_her'] == use_her]\n",
        "        if len(subset) > 0:\n",
        "            name = 'DQN + HER' if use_her else 'DQN (no HER)'\n",
        "            her_data[name] = subset\n",
        "\n",
        "    if her_data:\n",
        "        plot_learning_curves(\n",
        "            her_data,\n",
        "            'Tier 2: HER Ablation (Sparse Reward)',\n",
        "            os.path.join(config.log_dir, 'tier2_her_ablation.png')\n",
        "        )\n",
        "\n",
        "# Plot all methods comparison\n",
        "if len(tier1_df) > 0 or len(tier2_df) > 0 or len(tier3_df) > 0:\n",
        "    all_data = {}\n",
        "\n",
        "    if len(tier1_df) > 0:\n",
        "        all_data['Tier 1: DQN'] = tier1_df\n",
        "\n",
        "    if len(tier2_df) > 0:\n",
        "        her_subset = tier2_df[tier2_df['use_her'] == True]\n",
        "        no_her_subset = tier2_df[tier2_df['use_her'] == False]\n",
        "        if len(her_subset) > 0:\n",
        "            all_data['Tier 2: DQN+HER'] = her_subset\n",
        "        if len(no_her_subset) > 0:\n",
        "            all_data['Tier 2: DQN (no HER)'] = no_her_subset\n",
        "\n",
        "    if len(tier3_df) > 0:\n",
        "        all_data['Tier 3: HAC'] = tier3_df\n",
        "\n",
        "    if all_data:\n",
        "        plot_learning_curves(\n",
        "            all_data,\n",
        "            'Method Comparison: All Tiers',\n",
        "            os.path.join(config.log_dir, 'all_methods_comparison.png')\n",
        "        )"
      ],
      "metadata": {
        "id": "VoH7QTe-Numr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# EXPERIMENT SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def print_final_metrics(df: pd.DataFrame, name: str):\n",
        "    \"\"\"Print final metrics for a method.\"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        print(f\"\\n{name}: No data\")\n",
        "        return\n",
        "\n",
        "    # Get final timestep metrics averaged across seeds\n",
        "    final_timestep = df['timestep'].max()\n",
        "    final_data = df[df['timestep'] == final_timestep]\n",
        "\n",
        "    success_mean = final_data['success_rate'].mean()\n",
        "    success_std = final_data['success_rate'].std()\n",
        "    steps_mean = final_data['mean_steps'].mean()\n",
        "    efficiency_mean = final_data['mean_path_efficiency'].mean()\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Success Rate: {success_mean:.1%} Â± {success_std:.1%}\")\n",
        "    print(f\"  Mean Steps: {steps_mean:.1f}\")\n",
        "    print(f\"  Path Efficiency: {efficiency_mean:.2f}\")\n",
        "\n",
        "print_final_metrics(tier1_df, \"Tier 1: DQN (Dense Reward)\")\n",
        "\n",
        "if len(tier2_df) > 0:\n",
        "    no_her = tier2_df[tier2_df['use_her'] == False]\n",
        "    with_her = tier2_df[tier2_df['use_her'] == True]\n",
        "    print_final_metrics(no_her, \"Tier 2: DQN without HER (Sparse)\")\n",
        "    print_final_metrics(with_her, \"Tier 2: DQN with HER (Sparse)\")\n",
        "\n",
        "print_final_metrics(tier3_df, \"Tier 3: HAC\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "1. TIER 1 (DQN Backbone):\n",
        "   - Discrete actions make learning faster and more stable\n",
        "   - Dense reward shaping helps DQN learn in the maze environment\n",
        "\n",
        "2. TIER 2 (HER Ablation):\n",
        "   - WITHOUT HER: Agent struggles with sparse rewards (expected)\n",
        "   - WITH HER: Agent learns by relabeling failed trajectories as successes\n",
        "\n",
        "3. TIER 3 (HAC):\n",
        "   - Hierarchical decomposition enables longer-horizon planning\n",
        "   - High-level policy proposes subgoals, low-level executes\n",
        "\n",
        "EXPERIMENTAL NOTES:\n",
        "- [FAST MODE] Results shown with 1 seed and reduced training\n",
        "- [FULL MODE] For final report, use 3+ seeds and longer training\n",
        "\"\"\")\n",
        "\n",
        "# Save all results\n",
        "os.makedirs(config.log_dir, exist_ok=True)\n",
        "\n",
        "if len(tier1_df) > 0:\n",
        "    tier1_df.to_csv(os.path.join(config.log_dir, 'tier1_results.csv'), index=False)\n",
        "if len(tier2_df) > 0:\n",
        "    tier2_df.to_csv(os.path.join(config.log_dir, 'tier2_results.csv'), index=False)\n",
        "if len(tier3_df) > 0:\n",
        "    tier3_df.to_csv(os.path.join(config.log_dir, 'tier3_results.csv'), index=False)\n",
        "\n",
        "# Save config\n",
        "with open(os.path.join(config.log_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config.to_dict(), f, indent=2)\n",
        "\n",
        "print(f\"\\nAll results saved to: {config.log_dir}\")"
      ],
      "metadata": {
        "id": "Pwa1SQbXNwKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGTwI5goSkGl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}